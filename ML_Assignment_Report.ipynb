{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Estate Valuation in Sri Lanka: A Machine Learning Approach\n",
    "\n",
    "## 1. Introduction and Problem Identification\n",
    "**Problem**: Real estate and land valuation in Sri Lanka is often opaque, relying heavily on subjective appraisals and manual comparisons. There is a lack of accessible, data-driven tools for ordinary citizens to estimate land prices based on objective parameters like location and utility access.\n",
    "\n",
    "**Objective**: To develop a machine learning model capable of accurately predicting the price per perch of land in Sri Lanka. Provide an explainable AI (XAI) interface to ensure transparency in how the model derives its valuations.\n",
    "\n",
    "**Algorithm Selection**: While traditional linear models provide a baseline, this project utilizes **CatBoostRegressor** (Categorical Boosting). CatBoost is an advanced gradient boosting algorithm that handles categorical variables (like `District` and `City`) natively and effectively without requiring extensive one-hot encoding, reducing feature space sparsity and improving accuracy over standard algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Collection & Loading\n",
    "The dataset was custom compiled by web scraping real estate listings. It contains records of land sizes, locations, and binary indicators for essential utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries if running in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    print('Running in Google Colab. Installing dependencies...')\n",
    "    !pip install catboost shap\n",
    "except ImportError:\n",
    "    print('Not running in Google Colab. Continuing...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Load Custom Dataset\n",
    "import os\n",
    "file_path = 'data/processed/cleaned_land_data1.csv'\n",
    "if not os.path.exists(file_path):\n",
    "    print(\"Dataset not found at 'data/processed/cleaned_land_data1.csv'.\")\n",
    "    print(\"If you are in Google Colab, please upload 'cleaned_land_data1.csv' now.\")\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        uploaded = files.upload()\n",
    "        if uploaded:\n",
    "            file_path = list(uploaded.keys())[0]\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)\n",
    "Let's explore the distribution of land prices across major districts and understand the feature correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "top_districts = df['District'].value_counts().nlargest(10).index\n",
    "sns.boxplot(x='District', y='Price per perch', data=df[df['District'].isin(top_districts)])\n",
    "plt.title('Distribution of Price per Perch by Top 10 Districts')\n",
    "plt.yscale('log') # Log scale due to extreme premium outliers in Colombo\n",
    "plt.ylabel('Price per Perch (Log Scale LKR)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "# Calculate correlation matrix for numerical columns\n",
    "num_cols = df.select_dtypes(include=[np.number])\n",
    "sns.heatmap(num_cols.corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "We need to guarantee data types, separate the target variable (`Price per perch`), and split the data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Type checking\n",
    "df['District'] = df['District'].astype(str)\n",
    "df['City'] = df['City'].astype(str)\n",
    "df['Land size'] = df['Land size'].astype(float)\n",
    "df['Availability of electricity'] = df['Availability of electricity'].astype(int)\n",
    "df['Availability of tap water'] = df['Availability of tap water'].astype(int)\n",
    "\n",
    "# Define Target (y) and Features (X)\n",
    "y = df['Price per perch'].astype(float)\n",
    "X = df[['District', 'City', 'Land size', 'Availability of electricity', 'Availability of tap water']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training instances: {len(X_train)} | Testing instances: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Baseline Model Comparison (Random Forest)\n",
    "To justify the use of our advanced boosting technique (CatBoost), we must establish a baseline. We will use a standard `RandomForestRegressor`. Since standard Random Forest in scikit-learn cannot handle strings directly, we must apply an `OrdinalEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), ['District', 'City'])\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "rf_pred = rf_pipeline.predict(X_test)\n",
    "\n",
    "print(\"--- Random Forest Baseline Metrics ---\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, rf_pred)):.2f}\")\n",
    "print(f\"MAE:  {mean_absolute_error(y_test, rf_pred):.2f}\")\n",
    "print(f\"R2:   {r2_score(y_test, rf_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Model Training & Hyperparameter Tuning (CatBoost)\n",
    "We now train the `CatBoostRegressor`. We utilize `RandomizedSearchCV` to systematically tune the hyperparameters (`iterations`, `learning_rate`, `depth`) rather than relying on arbitrary guesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "categorical_features_indices = ['District', 'City']\n",
    "\n",
    "# Define base model\n",
    "cb_base = CatBoostRegressor(\n",
    "    random_seed=42, \n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Hyperparameter Selection Grid\n",
    "param_grid = {\n",
    "    'iterations': [200, 500],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'depth': [4, 6, 8]\n",
    "}\n",
    "\n",
    "print(\"Starting RandomizedSearchCV cross-validation...\")\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=cb_base,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=5,\n",
    "    cv=3,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train, cat_features=categorical_features_indices)\n",
    "best_params = random_search.best_params_\n",
    "print(f\"\\nOptimal tuned parameters found: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Final Model Evaluation\n",
    "Using the identified optimal hyperparameters, we assemble the final model. We also implement Early Stopping (`early_stopping_rounds=50`) to prevent overfitting on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = CatBoostRegressor(\n",
    "    iterations=best_params['iterations'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    depth=best_params['depth'],\n",
    "    cat_features=categorical_features_indices,\n",
    "    random_seed=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Fit with Early Stopping on the validation test set\n",
    "final_model.fit(X_train, y_train, eval_set=(X_test, y_test), early_stopping_rounds=50)\n",
    "\n",
    "cb_pred = final_model.predict(X_test)\n",
    "cb_rmse = np.sqrt(mean_squared_error(y_test, cb_pred))\n",
    "cb_mae = mean_absolute_error(y_test, cb_pred)\n",
    "cb_r2 = r2_score(y_test, cb_pred)\n",
    "\n",
    "print(\"--- Tuned CatBoost Final Metrics ---\")\n",
    "print(f\"RMSE: {cb_rmse:.2f}\")\n",
    "print(f\"MAE:  {cb_mae:.2f}\")\n",
    "print(f\"R2:   {cb_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Explainability (XAI) using SHAP\n",
    "Modern machine learning requires transparency. We apply **SHapley Additive exPlanations (SHAP)** to break down how the complex CatBoost model makes decisions.\n",
    "\n",
    "SHAP values measure the marginal contribution of each feature to the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs() # Initialize JavaScript visualization for SHAP inside notebooks\n",
    "\n",
    "# Create TreeExplainer tailored for Gradient Boosting trees\n",
    "explainer = shap.TreeExplainer(final_model)\n",
    "\n",
    "# Calculate SHAP values for the test set\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Summary Plot: Global Interpretability\n",
    "plt.title(\"Global Feature Importance (SHAP Summary Plot)\")\n",
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Local Explainability (XAI)\n",
    "Global interpretability tells us that `District` and `City` govern general value. But how does the model decide the price of *one specific property*?\n",
    "\n",
    "Below is a SHAP Force Plot. It shows the Base Value (average land price in the dataset), and how the specific features of an individual property push the value up (red) or down (blue) to arrive at the final output price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local XAI for the very first instance in our test array\n",
    "instance_idx = 0\n",
    "\n",
    "print(f\"Explaining prediction for:\\n{X_test.iloc[instance_idx]}\")\n",
    "print(f\"\\nPredicted Price: LKR {cb_pred[instance_idx]:,.2f}\")\n",
    "\n",
    "shap.force_plot(\n",
    "    explainer.expected_value, \n",
    "    shap_values[instance_idx,:], \n",
    "    X_test.iloc[instance_idx,:]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "- We successfully framed a real-world problem mapping locations and utilities to property prices.\n",
    "- We evaluated a Random Forest baseline model and subsequently improved upon it by utilizing CatBoostRegressor with RandomizedSearchCV.\n",
    "- Through SHAP (Global and Local XAI), we proved the model's transparency, demonstrating exactly how the prediction is heavily anchored geographically and marginally modified by utilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}